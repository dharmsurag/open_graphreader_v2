{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8681df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic\n",
      "  Downloading pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\n",
      "Collecting neo4j\n",
      "  Downloading neo4j-6.0.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.4 (from pydantic)\n",
      "  Downloading pydantic_core-2.41.4-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-extensions>=4.14.1 (from pydantic)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pytz (from neo4j)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.9.0-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Downloading pillow-12.0.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting shellingham (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: colorama in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading numpy-2.3.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading markupsafe-3.0.3-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading pydantic-2.12.3-py3-none-any.whl (462 kB)\n",
      "Downloading pydantic_core-2.41.4-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.3/2.0 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading neo4j-6.0.2-py3-none-any.whl (325 kB)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading torch-2.9.0-cp312-cp312-win_amd64.whl (109.3 MB)\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/109.3 MB 6.1 MB/s eta 0:00:18\n",
      "    --------------------------------------- 2.6/109.3 MB 6.3 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 3.9/109.3 MB 6.2 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 5.2/109.3 MB 6.1 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 6.6/109.3 MB 6.2 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 7.9/109.3 MB 6.2 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 9.2/109.3 MB 6.2 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 10.2/109.3 MB 6.2 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 6.2 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 6.2 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 6.2 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 15.5/109.3 MB 6.2 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 16.8/109.3 MB 6.2 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 18.1/109.3 MB 6.2 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 19.4/109.3 MB 6.2 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 20.7/109.3 MB 6.2 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 22.0/109.3 MB 6.2 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 23.3/109.3 MB 6.2 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 24.6/109.3 MB 6.2 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 26.0/109.3 MB 6.2 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 27.3/109.3 MB 6.2 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 28.6/109.3 MB 6.2 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 29.9/109.3 MB 6.2 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 31.2/109.3 MB 6.2 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 32.5/109.3 MB 6.2 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 33.6/109.3 MB 6.2 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 34.9/109.3 MB 6.2 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 36.2/109.3 MB 6.2 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 37.5/109.3 MB 6.2 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 38.8/109.3 MB 6.2 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 40.1/109.3 MB 6.2 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 41.4/109.3 MB 6.2 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 42.7/109.3 MB 6.2 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 44.0/109.3 MB 6.2 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 45.4/109.3 MB 6.2 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 46.9/109.3 MB 6.2 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 48.0/109.3 MB 6.2 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 49.3/109.3 MB 6.2 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 50.9/109.3 MB 6.2 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 52.2/109.3 MB 6.2 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 53.5/109.3 MB 6.2 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 54.8/109.3 MB 6.2 MB/s eta 0:00:09\n",
      "   -------------------- ------------------- 56.1/109.3 MB 6.2 MB/s eta 0:00:09\n",
      "   -------------------- ------------------- 57.1/109.3 MB 6.2 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 58.5/109.3 MB 6.2 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 59.8/109.3 MB 6.2 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 61.3/109.3 MB 6.2 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 62.7/109.3 MB 6.2 MB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 64.0/109.3 MB 6.2 MB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 65.3/109.3 MB 6.2 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 66.6/109.3 MB 6.2 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 67.9/109.3 MB 6.2 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 69.2/109.3 MB 6.2 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 70.8/109.3 MB 6.2 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 72.1/109.3 MB 6.2 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 73.4/109.3 MB 6.2 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 74.7/109.3 MB 6.2 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 76.0/109.3 MB 6.2 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 77.3/109.3 MB 6.2 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 78.6/109.3 MB 6.2 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 80.0/109.3 MB 6.2 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 81.5/109.3 MB 6.2 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 82.8/109.3 MB 6.2 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 84.1/109.3 MB 6.2 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 85.5/109.3 MB 6.2 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 86.8/109.3 MB 6.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 88.1/109.3 MB 6.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 89.7/109.3 MB 6.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 91.0/109.3 MB 6.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 92.3/109.3 MB 6.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 93.6/109.3 MB 6.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 94.9/109.3 MB 6.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 96.5/109.3 MB 6.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 97.8/109.3 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 99.1/109.3 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 100.4/109.3 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 101.7/109.3 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 103.3/109.3 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 104.3/109.3 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 105.9/109.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  107.0/109.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  108.3/109.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.1/109.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.1/109.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 109.3/109.3 MB 6.0 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.0 MB 6.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.6/12.0 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/12.0 MB 6.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.0 MB 6.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.6/12.0 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.9/12.0 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.4/12.0 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.0 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 4.8 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading pillow-12.0.0-cp312-cp312-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.3/7.0 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.6/7.0 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.9/7.0 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.2/7.0 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.8/7.0 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 6.1 MB/s eta 0:00:00\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.3/8.7 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.6/8.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.2/8.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.5/8.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.8/8.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.1/8.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading scipy-1.16.3-cp312-cp312-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.3/38.6 MB 6.7 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.6/38.6 MB 6.3 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 3.9/38.6 MB 6.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 5.2/38.6 MB 6.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 6.8/38.6 MB 6.3 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 8.1/38.6 MB 6.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 9.4/38.6 MB 6.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 10.7/38.6 MB 6.2 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 12.1/38.6 MB 6.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 13.4/38.6 MB 6.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 14.7/38.6 MB 6.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 16.0/38.6 MB 6.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 17.3/38.6 MB 6.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 18.6/38.6 MB 6.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 19.9/38.6 MB 6.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 21.2/38.6 MB 6.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 22.8/38.6 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 24.1/38.6 MB 6.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 25.4/38.6 MB 6.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 26.7/38.6 MB 6.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 28.0/38.6 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 29.4/38.6 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 30.7/38.6 MB 6.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 32.2/38.6 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 33.6/38.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 34.9/38.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.2/38.6 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.5/38.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 6.1 MB/s eta 0:00:00\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Downloading numpy-2.3.4-cp312-cp312-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.8 MB 6.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.6/12.8 MB 6.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.5/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.8/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.1/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.7/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 6.1 MB/s eta 0:00:00\n",
      "Downloading pyyaml-6.0.3-cp312-cp312-win_amd64.whl (154 kB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-win_amd64.whl (277 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.3/6.3 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.9/6.3 MB 6.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.3/2.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.7 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading markupsafe-3.0.3-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 4.4 MB/s eta 0:00:00\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Installing collected packages: pytz, mpmath, urllib3, typing-extensions, tqdm, threadpoolctl, sympy, setuptools, safetensors, regex, pyyaml, python-dotenv, Pillow, numpy, networkx, neo4j, MarkupSafe, joblib, idna, fsspec, filelock, charset_normalizer, certifi, annotated-types, typing-inspection, scipy, requests, pydantic-core, jinja2, torch, scikit-learn, pydantic, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed MarkupSafe-3.0.3 Pillow-12.0.0 annotated-types-0.7.0 certifi-2025.10.5 charset_normalizer-3.4.4 filelock-3.20.0 fsspec-2025.10.0 huggingface-hub-0.36.0 idna-3.11 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 neo4j-6.0.2 networkx-3.5 numpy-2.3.4 pydantic-2.12.3 pydantic-core-2.41.4 python-dotenv-1.2.1 pytz-2025.2 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.3 sentence-transformers-5.1.2 setuptools-80.9.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.1 torch-2.9.0 tqdm-4.67.1 transformers-4.57.1 typing-extensions-4.15.0 typing-inspection-0.4.2 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pydantic neo4j sentence-transformers python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58ea920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting litellm\n",
      "  Downloading litellm-1.79.1-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting aiohttp>=3.10 (from litellm)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting click (from litellm)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fastuuid>=0.13.0 (from litellm)\n",
      "  Downloading fastuuid-0.14.0-cp312-cp312-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting httpx>=0.23.0 (from litellm)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting importlib-metadata>=6.8.0 (from litellm)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from litellm) (3.1.6)\n",
      "Collecting jsonschema<5.0.0,>=4.22.0 (from litellm)\n",
      "  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting openai>=1.99.5 (from litellm)\n",
      "  Downloading openai-2.7.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from litellm) (2.12.3)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from litellm) (1.2.1)\n",
      "Collecting tiktoken>=0.7.0 (from litellm)\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tokenizers in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from litellm) (0.22.1)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.10->litellm)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp>=3.10->litellm)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp>=3.10->litellm)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.10->litellm)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.10->litellm)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp>=3.10->litellm)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp>=3.10->litellm)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-win_amd64.whl.metadata (77 kB)\n",
      "Collecting anyio (from httpx>=0.23.0->litellm)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: certifi in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from httpx>=0.23.0->litellm) (2025.10.5)\n",
      "Collecting httpcore==1.* (from httpx>=0.23.0->litellm)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from httpx>=0.23.0->litellm) (3.11)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.23.0->litellm)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=6.8.0->litellm)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.3)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.22.0->litellm)\n",
      "  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.22.0->litellm)\n",
      "  Downloading referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.22.0->litellm)\n",
      "  Downloading rpds_py-0.28.0-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.99.5->litellm)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai>=1.99.5->litellm)\n",
      "  Downloading jiter-0.11.1-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting sniffio (from openai>=1.99.5->litellm)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from openai>=1.99.5->litellm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from openai>=1.99.5->litellm) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2.32.5)\n",
      "Requirement already satisfied: colorama in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from click->litellm) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from tokenizers->litellm) (0.36.0)\n",
      "Requirement already satisfied: filelock in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (6.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ds_work\\graphreader_v2\\graphenv2\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.5.0)\n",
      "Downloading litellm-1.79.1-py3-none-any.whl (10.3 MB)\n",
      "   ---------------------------------------- 0.0/10.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.3/10.3 MB 6.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/10.3 MB 6.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.9/10.3 MB 6.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.0/10.3 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.3/10.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.6/10.3 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.9/10.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.3 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.3/10.3 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.13.2-cp312-cp312-win_amd64.whl (453 kB)\n",
      "Downloading fastuuid-0.14.0-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Downloading openai-2.7.1-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading tiktoken-0.12.0-cp312-cp312-win_amd64.whl (878 kB)\n",
      "   ---------------------------------------- 0.0/878.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 878.7/878.7 kB 5.6 MB/s eta 0:00:00\n",
      "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Downloading jiter-0.11.1-cp312-cp312-win_amd64.whl (204 kB)\n",
      "Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Downloading multidict-6.7.0-cp312-cp312-win_amd64.whl (46 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Downloading referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.28.0-cp312-cp312-win_amd64.whl (227 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-win_amd64.whl (87 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: zipp, sniffio, rpds-py, propcache, multidict, jiter, h11, frozenlist, fastuuid, distro, click, attrs, aiohappyeyeballs, yarl, tiktoken, referencing, importlib-metadata, httpcore, anyio, aiosignal, jsonschema-specifications, httpx, aiohttp, openai, jsonschema, litellm\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.11.0 attrs-25.4.0 click-8.3.0 distro-1.9.0 fastuuid-0.14.0 frozenlist-1.8.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 importlib-metadata-8.7.0 jiter-0.11.1 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 litellm-1.79.1 multidict-6.7.0 openai-2.7.1 propcache-0.4.1 referencing-0.37.0 rpds-py-0.28.0 sniffio-1.3.1 tiktoken-0.12.0 yarl-1.22.0 zipp-3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ingest-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DS_Work\\GraphReader_v2\\graphenv2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "import litellm\n",
    "from neo4j import GraphDatabase, Driver\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ingest-embedding-helper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper for Embeddings (Placeholder) ---\n",
    "# We'll use a placeholder. In production, you'd use a real model.\n",
    "# e.g., from sentence_transformers import SentenceTransformer\n",
    "# embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Placeholder for your embedding function.\n",
    "    Replace with your actual embedding model.\n",
    "    The dimension (e.g., 768) MUST match your index config.\n",
    "    \"\"\"\n",
    "    # In a real implementation:\n",
    "    embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    embeddings = embedding_model.encode(text)\n",
    "    return embeddings.tolist()\n",
    "    # return embedding_model.encode(text).tolist()\n",
    "    \n",
    "    # Using a 768-dim list of zeros as a placeholder\n",
    "    #print(f\"Generating placeholder embedding for: {text[:30]}...\")\n",
    "    #return [0.0] * 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ingest-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Load Config & Set Up Clients ===\n",
    "load_dotenv()\n",
    "\n",
    "# --- litellm Configuration ---\n",
    "litellm.api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "litellm.set_verbose = True\n",
    "#litellm._turn_on_debug()\n",
    "\n",
    "# This sets up your requested Groq -> Ollama fallback\n",
    "litellm.model_list = [\n",
    "    {\n",
    "        \"model_name\": \"groq/llama-3.3-70b-versatile\",\n",
    "        \"litellm_params\": {\n",
    "            \"model\": \"groq/llama-3.3-70b-versatile\",\n",
    "            \"api_key\": os.getenv(\"GROQ_API_KEY\")\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- Neo4j Connection ---\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USER\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "NEO4J_DATABASE = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ff864f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_name': 'groq/llama-3.3-70b-versatile',\n",
       "  'litellm_params': {'model': 'groq/llama-3.3-70b-versatile',\n",
       "   'api_key': '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'}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litellm.model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ingest-pydantic-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Pydantic Models for Ingestion ===\n",
    "# Based on our discussion and your original ingestor.ipynb\n",
    "\n",
    "class AtomicFact(BaseModel):\n",
    "    key_elements: List[str] = Field(..., description=\"Key nouns, verbs, or entities (e.g., 'Tesla', 'Q4 2023', '$1.2B')\")\n",
    "    atomic_fact: str = Field(..., description=\"A single, indivisible fact as a concise sentence.\")\n",
    "\n",
    "class ChunkEnrichment(BaseModel):\n",
    "    atomic_facts: List[AtomicFact]\n",
    "\n",
    "# New models for the generic graph structure\n",
    "class DocumentNode(BaseModel):\n",
    "    fileName: str\n",
    "    source_type: str = \"unstructured\"\n",
    "    content_hash: str # To avoid re-ingesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ingest-core-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Core Ingestion Functions ===\n",
    "\n",
    "def get_enrichment_from_chunk(chunk_text: str) -> Optional[ChunkEnrichment]:\n",
    "    \"\"\"\n",
    "    Calls the Groq LLM via litellm to extract facts.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an intelligent assistant. Meticulously extract structured information\n",
    "    from the following financial text.\n",
    "    \n",
    "    1. Key Elements: Essential nouns, entities, or numbers pivotal to the text.\n",
    "    2. Atomic Facts: The smallest, indivisible facts, presented as concise sentences.\n",
    "    \n",
    "    Format Instructions: {ChunkEnrichment.model_json_schema()} following the JSON schema.\n",
    "    \n",
    "    Text:\n",
    "    {chunk_text}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use litellm.completion to call Groq (or its fallback)\n",
    "        response = litellm.completion(\n",
    "            model=\"groq/llama-3.3-70b-versatile\", # Use the alias from model_list\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        json_response = response.choices[0].message.content\n",
    "        parsed_data = json.loads(json_response)\n",
    "        return ChunkEnrichment(**parsed_data)\n",
    "\n",
    "    except (Exception, ValidationError) as e:\n",
    "        # This triggers the fallback in the main ingestion loop\n",
    "        print(f\"Error during LLM enrichment or validation: {e}\")\n",
    "        return None\n",
    "\n",
    "def ingest_document(driver: Driver, doc: DocumentNode, chunks: List[str]):\n",
    "    \"\"\"\n",
    "    Ingests a document, its chunks, and enriched facts into Neo4j.\n",
    "    Implements the graceful fallback system.\n",
    "    \"\"\"\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        # 1. Create/Merge Document Node\n",
    "        session.run(\n",
    "            \"MERGE (d:Document {fileName: $fileName}) \"\n",
    "            \"ON CREATE SET d.content_hash = $hash, d.source_type = $type\",\n",
    "            fileName=doc.fileName, hash=doc.content_hash, type=doc.source_type\n",
    "        )\n",
    "        \n",
    "        print(f\"Ingesting document: {doc.fileName}\")\n",
    "        \n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            chunk_id = f\"{doc.fileName}_chunk_{i}\"\n",
    "            chunk_embedding = get_embedding(chunk_text)\n",
    "            \n",
    "            # 2. Create SectionChunk Node & Link to Document\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MATCH (d:Document {fileName: $fileName})\n",
    "                MERGE (s:SectionChunk {chunk_id: $chunk_id})\n",
    "                ON CREATE SET s.text = $text, s.embedding = $embedding\n",
    "                MERGE (d)-[:HAS_SECTION]->(s)\n",
    "                \"\"\",\n",
    "                fileName=doc.fileName, chunk_id=chunk_id, \n",
    "                text=chunk_text, embedding=chunk_embedding\n",
    "            )\n",
    "            \n",
    "            # 3. --- LLM Enrichment (Try/Except Fallback) ---\n",
    "            try:\n",
    "                enrichment = get_enrichment_from_chunk(chunk_text)\n",
    "                \n",
    "                if not enrichment or not enrichment.atomic_facts:\n",
    "                    # This is a soft failure, log it and move on\n",
    "                    raise Exception(\"LLM returned no valid atomic_facts.\")\n",
    "\n",
    "                # 4. If success, build the rich graph\n",
    "                for fact in enrichment.atomic_facts:\n",
    "                    fact_embedding = get_embedding(fact.atomic_fact)\n",
    "                    \n",
    "                    # Create FactNode, link to SectionChunk\n",
    "                    \n",
    "                    fact_id = session.run(\n",
    "                        \"\"\"\n",
    "                        MATCH (s:SectionChunk {chunk_id: $chunk_id})\n",
    "                        MERGE (f:FactNode {fact: $fact_text})\n",
    "                        ON CREATE SET f.embedding = $embedding\n",
    "                        MERGE (s)-[:HAS_FACT]->(f)\n",
    "                        RETURN elementId(f) AS fact_id\n",
    "                        \"\"\",\n",
    "                        chunk_id=chunk_id, \n",
    "                        fact_text=fact.atomic_fact, \n",
    "                        embedding=fact_embedding\n",
    "                    ).single()[\"fact_id\"]\n",
    "                    \n",
    "                    # Create EntityNodes, link to FactNode\n",
    "                    if fact.key_elements:\n",
    "                        session.run(\n",
    "                            \"\"\"\n",
    "                            MATCH (f:FactNode) WHERE elementId(f) = $fact_id\n",
    "                            WITH f\n",
    "                            UNWIND $elements AS elem_name\n",
    "                            MERGE (e:EntityNode {name: elem_name})\n",
    "                            MERGE (f)-[:HAS_ENTITY]->(e)\n",
    "                            \"\"\",\n",
    "                            fact_id=fact_id, elements=fact.key_elements\n",
    "                        )\n",
    "                print(f\"  + Enriched chunk {i} with {len(enrichment.atomic_facts)} facts.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # --- FALLBACK IN ACTION ---\n",
    "                # If enrichment fails, we just log it.\n",
    "                # The (Document)-[:HAS_SECTION]->(SectionChunk) link\n",
    "                # is already created and is sufficient for vector RAG.\n",
    "                print(f\"  ! Fallback: Skipped enrichment for chunk {i}: {e}\")\n",
    "                pass # Continue to the next chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ingest-create-indexes",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexes(driver: Driver):\n",
    "    \"\"\"\n",
    "    Creates Neo4j Vector and Full-Text indexes\n",
    "    (as per your https://neo4j.com/docs/cypher-manual/current/indexes/semantic-indexes/vector-indexes/ link)\n",
    "    \"\"\"\n",
    "    with driver.session(database=NEO4J_DATABASE) as session:\n",
    "        # 1. Vector index for GraphRAG entry (Facts)\n",
    "        session.run(\n",
    "            \"\"\"\n",
    "            CREATE VECTOR INDEX fact_embeddings IF NOT EXISTS\n",
    "            FOR (f:FactNode) ON (f.embedding)\n",
    "            OPTIONS { indexConfig: {\n",
    "              `vector.dimensions`: 768, \n",
    "              `vector.similarity_function`: 'cosine'\n",
    "            }}\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        # 2. Vector index for GraphRAG entry (Chunks - Fallback)\n",
    "        session.run(\n",
    "            \"\"\"\n",
    "            CREATE VECTOR INDEX section_embeddings IF NOT EXISTS\n",
    "            FOR (s:SectionChunk) ON (s.embedding)\n",
    "            OPTIONS { indexConfig: {\n",
    "              `vector.dimensions`: 768, \n",
    "              `vector.similarity_function`: 'cosine'\n",
    "            }}\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        # 3. Full-Text index for keyword search fallback\n",
    "        session.run(\n",
    "            \"\"\"\n",
    "            CREATE FULLTEXT INDEX text_index IF NOT EXISTS\n",
    "            FOR (n:FactNode|SectionChunk) ON EACH [n.fact, n.text]\n",
    "            \"\"\"\n",
    "        )\n",
    "        print(\"Indexes created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ingest-main-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Main Execution Function ===\n",
    "def main():\n",
    "    # --- Example Financial Document ---\n",
    "    doc_content = \"\"\"\n",
    "    Tesla, Inc. (TSLA) reported its fourth-quarter 2024 earnings on January 25, 2025.\n",
    "    The company announced total revenue of $25.17 billion, missing analyst expectations.\n",
    "    Automotive revenue was $21.56 billion. The Cybertruck production ramp-up\n",
    "    is proceeding, with 1,000 units built in a single week.\n",
    "    \n",
    "    For the full year 2024, Tesla delivered 1.81 million vehicles.\n",
    "    Net income (GAAP) for Q4 2024 was $2.48 billion.\n",
    "    The company warned of a notably lower volume growth rate in 2025.\n",
    "    \"\"\"\n",
    "    # Simple chunking for this example\n",
    "    chunks = doc_content.split(\"\\n\\n\") \n",
    "    \n",
    "    doc = DocumentNode(\n",
    "        fileName=\"tesla_q4_2024_earnings.txt\",\n",
    "        content_hash=hashlib.md5(doc_content.encode()).hexdigest()\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(\"Connecting to Neo4j...\")\n",
    "        print(\"Using database:\", NEO4J_DATABASE)\n",
    "        print(\"NEO4J_URI:\", NEO4J_URI)\n",
    "        print(\"NEO4J_USER:\", NEO4J_USER)\n",
    "        print(\"NEO4J_PASSWORD:\", NEO4J_PASSWORD)\n",
    "        driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "        driver.verify_connectivity()\n",
    "        print(\"Neo4j connection successful.\")\n",
    "        \n",
    "        # 1. Create indexes first\n",
    "        create_indexes(driver)\n",
    "        \n",
    "        # 2. Run ingestion\n",
    "        ingest_document(driver, doc, chunks)\n",
    "        \n",
    "        print(\"Ingestion complete.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        if 'driver' in locals():\n",
    "            driver.close()\n",
    "            print(\"Neo4j connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ingest-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Neo4j...\n",
      "Using database: graphreader2\n",
      "NEO4J_URI: neo4j://127.0.0.1:7687\n",
      "NEO4J_USER: neo4j\n",
      "NEO4J_PASSWORD: neo4j1999\n",
      "Neo4j connection successful.\n",
      "Indexes created successfully.\n",
      "Ingesting document: tesla_q4_2024_earnings.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m07:30:07 - LiteLLM:WARNING\u001b[0m: utils.py:551 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'fake_stream': True, 'response_format': {'type': 'json_object'}, 'extra_body': {}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=6, column=32, offset=277>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 277, 'line': 6, 'column': 32}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                        MATCH (s:SectionChunk {chunk_id: $chunk_id})\\n                        MERGE (f:FactNode {fact: $fact_text})\\n                        ON CREATE SET f.embedding = $embedding\\n                        MERGE (s)-[:HAS_FACT]->(f)\\n                        RETURN id(f) AS fact_id\\n                        '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=2, column=54, offset=54>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 54, 'line': 2, 'column': 54}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                            MATCH (f:FactNode) WHERE id(f) = $fact_id\\n                            WITH f\\n                            UNWIND $elements AS elem_name\\n                            MERGE (e:EntityNode {name: elem_name})\\n                            MERGE (f)-[:HAS_ENTITY]->(e)\\n                            '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=6, column=32, offset=277>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 277, 'line': 6, 'column': 32}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                        MATCH (s:SectionChunk {chunk_id: $chunk_id})\\n                        MERGE (f:FactNode {fact: $fact_text})\\n                        ON CREATE SET f.embedding = $embedding\\n                        MERGE (s)-[:HAS_FACT]->(f)\\n                        RETURN id(f) AS fact_id\\n                        '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=2, column=54, offset=54>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 54, 'line': 2, 'column': 54}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                            MATCH (f:FactNode) WHERE id(f) = $fact_id\\n                            WITH f\\n                            UNWIND $elements AS elem_name\\n                            MERGE (e:EntityNode {name: elem_name})\\n                            MERGE (f)-[:HAS_ENTITY]->(e)\\n                            '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=6, column=32, offset=277>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 277, 'line': 6, 'column': 32}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                        MATCH (s:SectionChunk {chunk_id: $chunk_id})\\n                        MERGE (f:FactNode {fact: $fact_text})\\n                        ON CREATE SET f.embedding = $embedding\\n                        MERGE (s)-[:HAS_FACT]->(f)\\n                        RETURN id(f) AS fact_id\\n                        '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=2, column=54, offset=54>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 54, 'line': 2, 'column': 54}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                            MATCH (f:FactNode) WHERE id(f) = $fact_id\\n                            WITH f\\n                            UNWIND $elements AS elem_name\\n                            MERGE (e:EntityNode {name: elem_name})\\n                            MERGE (f)-[:HAS_ENTITY]->(e)\\n                            '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=6, column=32, offset=277>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 277, 'line': 6, 'column': 32}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                        MATCH (s:SectionChunk {chunk_id: $chunk_id})\\n                        MERGE (f:FactNode {fact: $fact_text})\\n                        ON CREATE SET f.embedding = $embedding\\n                        MERGE (s)-[:HAS_FACT]->(f)\\n                        RETURN id(f) AS fact_id\\n                        '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=2, column=54, offset=54>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 54, 'line': 2, 'column': 54}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                            MATCH (f:FactNode) WHERE id(f) = $fact_id\\n                            WITH f\\n                            UNWIND $elements AS elem_name\\n                            MERGE (e:EntityNode {name: elem_name})\\n                            MERGE (f)-[:HAS_ENTITY]->(e)\\n                            '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=6, column=32, offset=277>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 277, 'line': 6, 'column': 32}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                        MATCH (s:SectionChunk {chunk_id: $chunk_id})\\n                        MERGE (f:FactNode {fact: $fact_text})\\n                        ON CREATE SET f.embedding = $embedding\\n                        MERGE (s)-[:HAS_FACT]->(f)\\n                        RETURN id(f) AS fact_id\\n                        '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  + Enriched chunk 0 with 5 facts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=2, column=54, offset=54>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 54, 'line': 2, 'column': 54}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                            MATCH (f:FactNode) WHERE id(f) = $fact_id\\n                            WITH f\\n                            UNWIND $elements AS elem_name\\n                            MERGE (e:EntityNode {name: elem_name})\\n                            MERGE (f)-[:HAS_ENTITY]->(e)\\n                            '\n",
      "\u001b[92m07:30:33 - LiteLLM:WARNING\u001b[0m: utils.py:551 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'fake_stream': True, 'response_format': {'type': 'json_object'}, 'extra_body': {}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=6, column=32, offset=277>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 277, 'line': 6, 'column': 32}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                        MATCH (s:SectionChunk {chunk_id: $chunk_id})\\n                        MERGE (f:FactNode {fact: $fact_text})\\n                        ON CREATE SET f.embedding = $embedding\\n                        MERGE (s)-[:HAS_FACT]->(f)\\n                        RETURN id(f) AS fact_id\\n                        '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=2, column=54, offset=54>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 54, 'line': 2, 'column': 54}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                            MATCH (f:FactNode) WHERE id(f) = $fact_id\\n                            WITH f\\n                            UNWIND $elements AS elem_name\\n                            MERGE (e:EntityNode {name: elem_name})\\n                            MERGE (f)-[:HAS_ENTITY]->(e)\\n                            '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=6, column=32, offset=277>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 277, 'line': 6, 'column': 32}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                        MATCH (s:SectionChunk {chunk_id: $chunk_id})\\n                        MERGE (f:FactNode {fact: $fact_text})\\n                        ON CREATE SET f.embedding = $embedding\\n                        MERGE (s)-[:HAS_FACT]->(f)\\n                        RETURN id(f) AS fact_id\\n                        '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=2, column=54, offset=54>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 54, 'line': 2, 'column': 54}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                            MATCH (f:FactNode) WHERE id(f) = $fact_id\\n                            WITH f\\n                            UNWIND $elements AS elem_name\\n                            MERGE (e:EntityNode {name: elem_name})\\n                            MERGE (f)-[:HAS_ENTITY]->(e)\\n                            '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=6, column=32, offset=277>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 277, 'line': 6, 'column': 32}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                        MATCH (s:SectionChunk {chunk_id: $chunk_id})\\n                        MERGE (f:FactNode {fact: $fact_text})\\n                        ON CREATE SET f.embedding = $embedding\\n                        MERGE (s)-[:HAS_FACT]->(f)\\n                        RETURN id(f) AS fact_id\\n                        '\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N01', status_description='warn: feature deprecated with replacement. id is deprecated. It is replaced by elementId or an application-generated id.', position=<SummaryInputPosition line=2, column=54, offset=54>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 54, 'line': 2, 'column': 54}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\\n                            MATCH (f:FactNode) WHERE id(f) = $fact_id\\n                            WITH f\\n                            UNWIND $elements AS elem_name\\n                            MERGE (e:EntityNode {name: elem_name})\\n                            MERGE (f)-[:HAS_ENTITY]->(e)\\n                            '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  + Enriched chunk 1 with 3 facts.\n",
      "Ingestion complete.\n",
      "Neo4j connection closed.\n"
     ]
    }
   ],
   "source": [
    "# This will run the main ingestion function\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba4c641d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m07:30:45 - LiteLLM:WARNING\u001b[0m: utils.py:551 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'fake_stream': True, 'response_format': {'type': 'json_object'}, 'extra_body': {}}\n",
      "atomic_facts=[AtomicFact(key_elements=['Tesla', 'Q4 2024', 'January 25, 2025'], atomic_fact='Tesla reported its Q4 2024 earnings on January 25, 2025.'), AtomicFact(key_elements=['TSLA', 'Tesla, Inc.', 'fourth-quarter 2024'], atomic_fact='Tesla, Inc. is represented by the stock symbol TSLA.'), AtomicFact(key_elements=['January 25, 2025', 'Q4 2024 earnings'], atomic_fact='The Q4 2024 earnings of Tesla were reported on January 25, 2025.')]\n"
     ]
    }
   ],
   "source": [
    "sample_chunk = \"Tesla, Inc. (TSLA) reported its fourth-quarter 2024 earnings on January 25, 2025.\"\n",
    "result = get_enrichment_from_chunk(sample_chunk)\n",
    "print(result)  # Should output a validated ChunkEnrichment object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
